{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [markdown]\n# # Import Libraries\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:48:18.245260Z\",\"iopub.execute_input\":\"2023-05-25T22:48:18.245652Z\",\"iopub.status.idle\":\"2023-05-25T22:48:26.151855Z\",\"shell.execute_reply.started\":\"2023-05-25T22:48:18.245619Z\",\"shell.execute_reply\":\"2023-05-25T22:48:26.150435Z\"}}\n# Data Collection, Data Cleaning & Data Manipulation\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\n\n# Data Visualization\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data Transformation\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom scipy import stats\n\n# Models Building\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n\n## Classification Problems\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\n## Regression Problems\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDRegressor\nfrom sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Explainbale AI (XAI)\n\nimport lime.lime_tabular\nimport shap\n\n# Unsupervised Learning: Clustering\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, Birch, MeanShift, SpectralClustering\nfrom sklearn.metrics import adjusted_rand_score\nimport warnings \nwarnings.filterwarnings('ignore')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:48:26.153634Z\",\"iopub.execute_input\":\"2023-05-25T22:48:26.154230Z\",\"iopub.status.idle\":\"2023-05-25T22:48:26.228485Z\",\"shell.execute_reply.started\":\"2023-05-25T22:48:26.154199Z\",\"shell.execute_reply\":\"2023-05-25T22:48:26.226840Z\"}}\ndf_churn = pd.read_csv('/kaggle/input/telecom-churn-data/Telecom_Churn_Dataset.csv')\n\n# %% [markdown]\n# # Utilities\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:48:26.230726Z\",\"iopub.execute_input\":\"2023-05-25T22:48:26.231173Z\",\"iopub.status.idle\":\"2023-05-25T22:48:26.346002Z\",\"shell.execute_reply.started\":\"2023-05-25T22:48:26.231143Z\",\"shell.execute_reply\":\"2023-05-25T22:48:26.344007Z\"}}\n# Makes sure we see all columns\npd.set_option('display.max_columns', None)\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import RandomOverSampler\n\nclass DataLoader():\n    def __init__(self):\n        self.data = None\n\n    def load_dataset(self, path=\"/kaggle/input/telecom-churn-data/Telecom_Churn_Dataset.csv\"):\n        self.data = pd.read_csv(path)\n        #Changing Datatypes\n        self.data['TotalCharges'] = pd.to_numeric(self.data['TotalCharges'], errors='coerce').astype(float)\n        # Drop id as it is not relevant\n         # Impute missing values of BMI\n        self.data.TotalCharges = self.data.TotalCharges.fillna(0)\n        self.data.drop([\"customerID\"], axis=1, inplace=True)\n        # Mapping the Target Variable\n        target_mapping = {\"Yes\": 1, \"No\": 0}\n        self.data['Churn'] = self.data['Churn'].map(target_mapping)\n\n    def standardize(self):\n\n        # Standardization \n        from sklearn.preprocessing import StandardScaler\n        from sklearn.compose import ColumnTransformer\n        from sklearn.pipeline import Pipeline\n\n        features = self.data.drop('Churn', axis=1)\n        target = self.data['Churn']\n\n        # Separate the numerical and categorical columns\n        numerical_cols = features.select_dtypes(include=['float64', 'int64']).columns\n        categorical_cols = features.select_dtypes(include=['object']).columns\n\n        # Define the feature scaling transformer\n        numerical_transformer = Pipeline([\n            ('scaler', StandardScaler())\n        ])\n\n        # Define the column transformer to apply different transformations to numerical and categorical columns\n        preprocessor = ColumnTransformer(\n            transformers=[\n                ('num', numerical_transformer, numerical_cols)\n                # Add more transformers if needed for categorical or other types of columns\n            ])\n\n        # Apply the preprocessing steps to the features only\n        transformed_data = preprocessor.fit_transform(features)\n\n        # If you want to convert the transformed data back to a DataFrame\n        self.data = pd.DataFrame(transformed_data, columns=numerical_cols)\n        self.data[categorical_cols] = features[categorical_cols]  # Include the categorical columns as they are\n        self.data['Churn'] = target  # Include the target variable in the transformed DataFrame\n        \n    def preprocess_data(self):\n        # One-hot encode all categorical columns\n        categorical_cols = [\"gender\",\n                            \"Partner\",\n                            \"Dependents\",\n                            \"PhoneService\",\n                            \"MultipleLines\",\n                            \"InternetService\",\n                            \"OnlineBackup\",\n                            \"OnlineSecurity\",\n                            \"DeviceProtection\",\n                            \"TechSupport\",\n                            \"StreamingTV\",\n                            \"StreamingMovies\",\n                            \"Contract\",\n                            \"PaperlessBilling\",\n                            \"PaymentMethod\"]\n        \n        encoded = pd.get_dummies(self.data[categorical_cols], \n                                prefix=categorical_cols)\n        \n        # Update data with new columns\n        self.data = pd.concat([encoded, self.data], axis=1)\n        self.data.drop(categorical_cols, axis=1, inplace=True)\n        \n\n    def get_data_split(self):\n        X = self.data.iloc[:,:-1]\n        y = self.data.iloc[:,-1]\n        return train_test_split(X, y, test_size=0.20, random_state=2021)\n    \n    def oversample(self, X_train, y_train):\n        oversample = RandomOverSampler(sampling_strategy='minority')\n        # Convert to numpy and oversample\n        x_np = X_train.to_numpy()\n        y_np = y_train.to_numpy()\n        x_np, y_np = oversample.fit_resample(x_np, y_np)\n        # Convert back to pandas\n        x_over = pd.DataFrame(x_np, columns=X_train.columns)\n        y_over = pd.Series(y_np, name=y_train.name)\n        return x_over, y_over\n   \n        \n\n# %% [markdown]\n# # Data Exploration\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:48:26.349761Z\",\"iopub.execute_input\":\"2023-05-25T22:48:26.350208Z\",\"iopub.status.idle\":\"2023-05-25T22:48:26.436733Z\",\"shell.execute_reply.started\":\"2023-05-25T22:48:26.350168Z\",\"shell.execute_reply\":\"2023-05-25T22:48:26.434859Z\"}}\nimport matplotlib.pyplot as plt\n\n\n# Load data\ndata_loader = DataLoader()\ndata_loader.load_dataset()\ndata = data_loader.data\n\n# Show head\nprint(data.shape)\ndata.head()\n\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:48:26.438177Z\",\"iopub.execute_input\":\"2023-05-25T22:48:26.438599Z\",\"iopub.status.idle\":\"2023-05-25T22:48:26.478768Z\",\"shell.execute_reply.started\":\"2023-05-25T22:48:26.438569Z\",\"shell.execute_reply\":\"2023-05-25T22:48:26.477291Z\"}}\n#  Show general statistics\ndata.info()\n\n# %% [code] {\"_kg_hide-output\":false,\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:48:26.480040Z\",\"iopub.execute_input\":\"2023-05-25T22:48:26.480398Z\",\"iopub.status.idle\":\"2023-05-25T22:48:29.067456Z\",\"shell.execute_reply.started\":\"2023-05-25T22:48:26.480372Z\",\"shell.execute_reply\":\"2023-05-25T22:48:29.066580Z\"}}\n# Show histogram for all columns\ncolumns = data.columns\nfor col in columns:\n    print(\"col: \", col)\n    data[col].hist()\n    plt.show()\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:48:29.068593Z\",\"iopub.execute_input\":\"2023-05-25T22:48:29.069901Z\",\"iopub.status.idle\":\"2023-05-25T22:48:29.117278Z\",\"shell.execute_reply.started\":\"2023-05-25T22:48:29.069871Z\",\"shell.execute_reply\":\"2023-05-25T22:48:29.116423Z\"}}\n# Show Standardize dataframe\ndata_loader.standardize()\ndata_loader.data.head()\n\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:48:29.118532Z\",\"iopub.execute_input\":\"2023-05-25T22:48:29.119028Z\",\"iopub.status.idle\":\"2023-05-25T22:48:29.127248Z\",\"shell.execute_reply.started\":\"2023-05-25T22:48:29.118998Z\",\"shell.execute_reply\":\"2023-05-25T22:48:29.125913Z\"}}\ndata_loader.data.dtypes\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:48:29.128641Z\",\"iopub.execute_input\":\"2023-05-25T22:48:29.129002Z\",\"iopub.status.idle\":\"2023-05-25T22:48:29.185242Z\",\"shell.execute_reply.started\":\"2023-05-25T22:48:29.128974Z\",\"shell.execute_reply\":\"2023-05-25T22:48:29.184238Z\"}}\ndata_loader.preprocess_data()\ndata_loader.data.head()\n\n# %% [markdown]\n# # Feature Importance\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:48:29.189336Z\",\"iopub.execute_input\":\"2023-05-25T22:48:29.189659Z\",\"iopub.status.idle\":\"2023-05-25T22:48:29.203451Z\",\"shell.execute_reply.started\":\"2023-05-25T22:48:29.189631Z\",\"shell.execute_reply\":\"2023-05-25T22:48:29.201654Z\"}}\n# Split the data for evaluation\nX_train, X_test, y_train, y_test = data_loader.get_data_split()\nprint(X_train.shape)\nprint(X_test.shape)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:48:29.205317Z\",\"iopub.execute_input\":\"2023-05-25T22:48:29.205841Z\",\"iopub.status.idle\":\"2023-05-25T22:48:30.520147Z\",\"shell.execute_reply.started\":\"2023-05-25T22:48:29.205803Z\",\"shell.execute_reply\":\"2023-05-25T22:48:30.519187Z\"}}\n\nclf = RandomForestClassifier()\nclf.fit(X_train, y_train)\n\n# Get feature importance scores\nfeature_importances = clf.feature_importances_\n\n# Create a dataframe to store feature importance\nfeature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances})\n\n# Sort the dataframe by importance scores in descending order\nfeature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n\n# Plot the feature importance scores\nplt.figure(figsize=(12, 10))\nsns.barplot(x='Importance', y='Feature', data=feature_importance_df)\nplt.title('Feature Importance')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.show()\n\n# %% [markdown]\n# **Using Tree-Based Feature Selection Technique**\n\n# %% [markdown]\n# # AutoML Using PyCaret\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:48:30.521392Z\",\"iopub.execute_input\":\"2023-05-25T22:48:30.521855Z\",\"iopub.status.idle\":\"2023-05-25T22:50:08.478674Z\",\"shell.execute_reply.started\":\"2023-05-25T22:48:30.521826Z\",\"shell.execute_reply\":\"2023-05-25T22:50:08.477611Z\"}}\npip install pycaret[full]\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:50:08.479872Z\",\"iopub.execute_input\":\"2023-05-25T22:50:08.480231Z\",\"iopub.status.idle\":\"2023-05-25T22:50:08.515035Z\",\"shell.execute_reply.started\":\"2023-05-25T22:50:08.480195Z\",\"shell.execute_reply\":\"2023-05-25T22:50:08.513248Z\"}}\nimport pycaret\npycaret.__version__\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:50:08.517501Z\",\"iopub.execute_input\":\"2023-05-25T22:50:08.518283Z\",\"iopub.status.idle\":\"2023-05-25T22:50:09.972814Z\",\"shell.execute_reply.started\":\"2023-05-25T22:50:08.518255Z\",\"shell.execute_reply\":\"2023-05-25T22:50:09.970905Z\"}}\n# read csv data\ndata = pd.read_csv('https://raw.githubusercontent.com/srees1988/predict-churn-py/main/customer_churn_data.csv')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:50:09.974331Z\",\"iopub.execute_input\":\"2023-05-25T22:50:09.974716Z\",\"iopub.status.idle\":\"2023-05-25T22:50:09.982950Z\",\"shell.execute_reply.started\":\"2023-05-25T22:50:09.974657Z\",\"shell.execute_reply\":\"2023-05-25T22:50:09.981453Z\"}}\ndata.dtypes\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:50:09.984384Z\",\"iopub.execute_input\":\"2023-05-25T22:50:09.984742Z\",\"iopub.status.idle\":\"2023-05-25T22:50:10.003726Z\",\"shell.execute_reply.started\":\"2023-05-25T22:50:09.984694Z\",\"shell.execute_reply\":\"2023-05-25T22:50:10.001558Z\"}}\n# replace blanks with np.nan\ndata['TotalCharges'] = data['TotalCharges'].replace(' ', np.nan)\n\n# convert to float64\ndata['TotalCharges'] = data['TotalCharges'].astype('float64')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:50:10.006386Z\",\"iopub.execute_input\":\"2023-05-25T22:50:10.006894Z\",\"iopub.status.idle\":\"2023-05-25T22:50:19.789874Z\",\"shell.execute_reply.started\":\"2023-05-25T22:50:10.006851Z\",\"shell.execute_reply\":\"2023-05-25T22:50:19.788715Z\"}}\n# init setup**\nfrom pycaret.classification import *\nclf1 = setup(data, target = 'Churn', ignore_features = ['customerID'])\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:50:19.791144Z\",\"iopub.execute_input\":\"2023-05-25T22:50:19.791411Z\",\"iopub.status.idle\":\"2023-05-25T22:52:18.771246Z\",\"shell.execute_reply.started\":\"2023-05-25T22:50:19.791389Z\",\"shell.execute_reply\":\"2023-05-25T22:52:18.769737Z\"}}\n# compare all models**\nbest_model = compare_models(sort='AUC')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:52:18.773055Z\",\"iopub.execute_input\":\"2023-05-25T22:52:18.773468Z\",\"iopub.status.idle\":\"2023-05-25T22:52:18.779281Z\",\"shell.execute_reply.started\":\"2023-05-25T22:52:18.773436Z\",\"shell.execute_reply\":\"2023-05-25T22:52:18.778305Z\"}}\n# print best_model parameters**\nprint(best_model)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:52:18.781151Z\",\"iopub.execute_input\":\"2023-05-25T22:52:18.781568Z\",\"iopub.status.idle\":\"2023-05-25T22:55:39.774863Z\",\"shell.execute_reply.started\":\"2023-05-25T22:52:18.781538Z\",\"shell.execute_reply\":\"2023-05-25T22:55:39.773796Z\"}}\n# tune best model**\ntuned_best_model = tune_model(best_model)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:55:39.776989Z\",\"iopub.execute_input\":\"2023-05-25T22:55:39.777349Z\",\"iopub.status.idle\":\"2023-05-25T22:55:40.599786Z\",\"shell.execute_reply.started\":\"2023-05-25T22:55:39.777318Z\",\"shell.execute_reply\":\"2023-05-25T22:55:40.598061Z\"}}\n# AUC Plot**\nplot_model(tuned_best_model, plot = 'auc')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:55:40.604699Z\",\"iopub.execute_input\":\"2023-05-25T22:55:40.605615Z\",\"iopub.status.idle\":\"2023-05-25T22:55:41.220673Z\",\"shell.execute_reply.started\":\"2023-05-25T22:55:40.605569Z\",\"shell.execute_reply\":\"2023-05-25T22:55:41.219726Z\"}}\n#Feature Importance Plot\nplot_model(tuned_best_model, plot = 'feature')\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:55:41.222550Z\",\"iopub.execute_input\":\"2023-05-25T22:55:41.223040Z\",\"iopub.status.idle\":\"2023-05-25T22:55:41.984563Z\",\"shell.execute_reply.started\":\"2023-05-25T22:55:41.223007Z\",\"shell.execute_reply\":\"2023-05-25T22:55:41.982729Z\"}}\n# Confusion Matrix**\nplot_model(tuned_best_model, plot = 'confusion_matrix')\n\n\n# %% [markdown]\n# ### Comparing 4 Best Models:\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:55:41.985959Z\",\"iopub.execute_input\":\"2023-05-25T22:55:41.986309Z\",\"iopub.status.idle\":\"2023-05-25T22:56:58.308330Z\",\"shell.execute_reply.started\":\"2023-05-25T22:55:41.986279Z\",\"shell.execute_reply\":\"2023-05-25T22:56:58.307127Z\"}}\n# compare models\nbest = compare_models(n_select = 4)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:56:58.310147Z\",\"iopub.execute_input\":\"2023-05-25T22:56:58.310509Z\",\"iopub.status.idle\":\"2023-05-25T22:56:58.317883Z\",\"shell.execute_reply.started\":\"2023-05-25T22:56:58.310480Z\",\"shell.execute_reply\":\"2023-05-25T22:56:58.316385Z\"}}\ntype(best)\n# >>> list\n\nprint(best)\n\n# %% [markdown]\n# GaussianNB(priors=None, var_smoothing=1e-09)\n# \n# \n# KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n#                      metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n#                      weights='uniform')\n# \n# LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n#                    intercept_scaling=1, l1_ratio=None, max_iter=1000,\n#                    multi_class='auto', n_jobs=None, penalty='l2',\n#                    random_state=4733, solver='lbfgs', tol=0.0001, verbose=0,\n#                    warm_start=False)\n# \n# RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n#                        criterion='gini', max_depth=None, max_features='sqrt',\n#                        max_leaf_nodes=None, max_samples=None,\n#                        min_impurity_decrease=0.0, min_samples_leaf=1,\n#                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n#                        n_estimators=100, n_jobs=-1, oob_score=False,\n#                        random_state=4733, verbose=0, warm_start=False)]\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:56:58.319416Z\",\"iopub.execute_input\":\"2023-05-25T22:56:58.319763Z\",\"iopub.status.idle\":\"2023-05-25T22:58:12.697917Z\",\"shell.execute_reply.started\":\"2023-05-25T22:56:58.319733Z\",\"shell.execute_reply\":\"2023-05-25T22:58:12.696359Z\"}}\nbest = compare_models(n_select = 4, sort='auc')\n\n# %% [markdown]\n# **LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n#                    intercept_scaling=1, l1_ratio=None, max_iter=1000,\n#                    multi_class='auto', n_jobs=None, penalty='l2',\n#                    random_state=4733, solver='lbfgs', tol=0.0001, verbose=0,\n#                    warm_start=False)**\n#                    \n# **GaussianNB(priors=None, var_smoothing=1e-09)**\n# \n# **GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n#                            learning_rate=0.1, loss='log_loss', max_depth=3,\n#                            max_features=None, max_leaf_nodes=None,\n#                            min_impurity_decrease=0.0, min_samples_leaf=1,\n#                            min_samples_split=2, min_weight_fraction_leaf=0.0,\n#                            n_estimators=100, n_iter_no_change=None,\n#                            random_state=4432, subsample=1.0, tol=0.0001,\n#                            validation_fraction=0.1, verbose=0,\n#                            warm_start=False)**\n# \n# **RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n#                        criterion='gini', max_depth=None, max_features='sqrt',\n#                        max_leaf_nodes=None, max_samples=None,\n#                        min_impurity_decrease=0.0, min_samples_leaf=1,\n#                        min_samples_split=2, min_weight_fraction_leaf=0.0,\n#                        n_estimators=100, n_jobs=-1, oob_score=False,\n#                        random_state=4733, verbose=0, warm_start=False)]**\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:58:12.699466Z\",\"iopub.execute_input\":\"2023-05-25T22:58:12.699887Z\",\"iopub.status.idle\":\"2023-05-25T22:58:12.707614Z\",\"shell.execute_reply.started\":\"2023-05-25T22:58:12.699849Z\",\"shell.execute_reply\":\"2023-05-25T22:58:12.706332Z\"}}\ntype(best)\n# >>> list\n\nprint(best)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:58:12.715087Z\",\"iopub.execute_input\":\"2023-05-25T22:58:12.715417Z\",\"iopub.status.idle\":\"2023-05-25T22:58:12.724140Z\",\"shell.execute_reply.started\":\"2023-05-25T22:58:12.715392Z\",\"shell.execute_reply\":\"2023-05-25T22:58:12.722728Z\"}}\n## Split the data for evaluation\n#X_train, X_test, y_train, y_test = data_loader.get_data_split()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:58:12.726493Z\",\"iopub.execute_input\":\"2023-05-25T22:58:12.727068Z\",\"iopub.status.idle\":\"2023-05-25T22:58:13.002100Z\",\"shell.execute_reply.started\":\"2023-05-25T22:58:12.727025Z\",\"shell.execute_reply\":\"2023-05-25T22:58:13.000525Z\"}}\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_curve, roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = data_loader.get_data_split()\n\n# Define the logistic regression model with the given hyperparameters\nlogreg = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                            intercept_scaling=1, l1_ratio=None, max_iter=1000,\n                            multi_class='auto', n_jobs=None, penalty='l2',\n                            random_state=4733, solver='lbfgs', tol=0.0001,\n                            verbose=0, warm_start=False)\n\n# Train the logistic regression model\nlogreg.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = logreg.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Calculate the F1 score\nf1 = f1_score(y_test, y_pred)\nprint(\"F1 Score:\", f1)\n\n# Calculate the recall score\nrecall = recall_score(y_test, y_pred)\nprint(\"Recall:\", recall)\n\n# Calculate the precision score\nprecision = precision_score(y_test, y_pred)\nprint(\"Precision:\", precision)\n\n# Calculate ROC curve and AUC score\ny_prob = logreg.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nauc = roc_auc_score(y_test, y_prob)\nprint(\"AUC Score:\", auc)\n\n# Plot ROC curve\nimport matplotlib.pyplot as plt\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.show()\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:58:13.007087Z\",\"iopub.execute_input\":\"2023-05-25T22:58:13.007502Z\",\"iopub.status.idle\":\"2023-05-25T22:58:13.209359Z\",\"shell.execute_reply.started\":\"2023-05-25T22:58:13.007469Z\",\"shell.execute_reply\":\"2023-05-25T22:58:13.207726Z\"}}\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, precision_score\n\n# Assuming you have your features and labels in X and y respectively\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = data_loader.get_data_split()\n\n# Create a Gaussian Naive Bayes model with specified hyperparameters\nmodel = GaussianNB(priors=None, var_smoothing=1e-09)\n\n# Train the model on the training data\nmodel.fit(X_train, y_train)\n\n# Predict the labels for the test set\ny_pred = model.predict(X_test)\n\n# Calculate the accuracy of the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Calculate the ROC AUC score\n#roc_auc = roc_auc_score(y_test, y_pred)\n#print(\"ROC AUC:\", roc_auc)\n\n# Calculate the F1 score\nf1 = f1_score(y_test, y_pred)\nprint(\"F1 Score:\", f1)\n\n# Calculate the recall score\nrecall = recall_score(y_test, y_pred)\nprint(\"Recall:\", recall)\n\n# Calculate the precision score\nprecision = precision_score(y_test, y_pred)\nprint(\"Precision:\", precision)\n\n# Calculate ROC curve and AUC score\ny_prob = model.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nauc = roc_auc_score(y_test, y_prob)\nprint(\"AUC Score:\", auc)\n\n# Plot ROC curve\nimport matplotlib.pyplot as plt\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:58:13.210893Z\",\"iopub.execute_input\":\"2023-05-25T22:58:13.211234Z\",\"iopub.status.idle\":\"2023-05-25T22:58:14.608261Z\",\"shell.execute_reply.started\":\"2023-05-25T22:58:13.211204Z\",\"shell.execute_reply\":\"2023-05-25T22:58:14.606714Z\"}}\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, precision_score\n\n# Assuming you have your features and labels in X and y respectively\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = data_loader.get_data_split()\n\n# Create a Gradient Boosting Classifier with specified hyperparameters\nmodel = GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None, learning_rate=0.1, loss='log_loss', max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, random_state=4432, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False)\n\n# Train the model on the training data\nmodel.fit(X_train, y_train)\n\n# Predict the labels for the test set\ny_pred = model.predict(X_test)\n\n# Calculate the accuracy of the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Calculate the F1 score\nf1 = f1_score(y_test, y_pred)\nprint(\"F1 Score:\", f1)\n\n# Calculate the recall score\nrecall = recall_score(y_test, y_pred)\nprint(\"Recall:\", recall)\n\n# Calculate the precision score\nprecision = precision_score(y_test, y_pred)\nprint(\"Precision:\", precision)\n\n# Calculate ROC curve and AUC score\ny_prob = model.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nauc = roc_auc_score(y_test, y_prob)\nprint(\"AUC Score:\", auc)\n\n# Plot ROC curve\nimport matplotlib.pyplot as plt\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.show()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:58:14.610502Z\",\"iopub.execute_input\":\"2023-05-25T22:58:14.610917Z\",\"iopub.status.idle\":\"2023-05-25T22:58:15.487614Z\",\"shell.execute_reply.started\":\"2023-05-25T22:58:14.610884Z\",\"shell.execute_reply\":\"2023-05-25T22:58:15.486129Z\"}}\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, precision_score\n\n# Assuming you have your features and labels in X and y respectively\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = data_loader.get_data_split()\n\n# Create a RandomForestClassifier with specified hyperparameters\nmodel = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=None, max_features='sqrt', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1, oob_score=False, random_state=4733, verbose=0, warm_start=False)\n\n# Train the model on the training data\nmodel.fit(X_train, y_train)\n\n# Predict the labels for the test set\ny_pred = model.predict(X_test)\n\n# Calculate the accuracy of the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n# Calculate the F1 score\nf1 = f1_score(y_test, y_pred)\nprint(\"F1 Score:\", f1)\n\n# Calculate the recall score\nrecall = recall_score(y_test, y_pred)\nprint(\"Recall:\", recall)\n\n# Calculate the precision score\nprecision = precision_score(y_test, y_pred)\nprint(\"Precision:\", precision)\n\n# Calculate ROC curve and AUC score\ny_prob = model.predict_proba(X_test)[:, 1]\nfpr, tpr, thresholds = roc_curve(y_test, y_prob)\nauc = roc_auc_score(y_test, y_prob)\nprint(\"AUC Score:\", auc)\n\n# Plot ROC curve\nimport matplotlib.pyplot as plt\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.show()\n\n# %% [markdown]\n# # LIME\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:58:15.489699Z\",\"iopub.execute_input\":\"2023-05-25T22:58:15.490682Z\",\"iopub.status.idle\":\"2023-05-25T22:58:17.069798Z\",\"shell.execute_reply.started\":\"2023-05-25T22:58:15.490615Z\",\"shell.execute_reply\":\"2023-05-25T22:58:17.068181Z\"}}\npip install interpret\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T22:58:17.071498Z\",\"iopub.execute_input\":\"2023-05-25T22:58:17.071980Z\",\"iopub.status.idle\":\"2023-05-25T22:58:17.108919Z\",\"shell.execute_reply.started\":\"2023-05-25T22:58:17.071931Z\",\"shell.execute_reply\":\"2023-05-25T22:58:17.107247Z\"}}\n#Imports\nfrom interpret.glassbox import (LogisticRegression,\n                                ClassificationTree, \n                                ExplainableBoostingClassifier)\nfrom interpret.glassbox import ExplainableBoostingClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, precision_score\nfrom interpret.blackbox import LimeTabular\nimport interpret\nfrom interpret import show\n\n# Split the data for evaluation\n#X_train, X_test, y_train, y_test = data_loader.get_data_split()\n# Oversample the train data\nX_train, y_train = data_loader.oversample(X_train, y_train)\n\n# %% [markdown]\n# ## 1) Applying Lime Using Random Forest\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T23:37:10.881555Z\",\"iopub.execute_input\":\"2023-05-25T23:37:10.882557Z\",\"iopub.status.idle\":\"2023-05-25T23:37:11.629061Z\",\"shell.execute_reply.started\":\"2023-05-25T23:37:10.882505Z\",\"shell.execute_reply\":\"2023-05-25T23:37:11.628053Z\"}}\n# %% Fit blackbox model\nimport lime\nimport lime.lime_tabular\nrf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=None, max_features='sqrt', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1, oob_score=False, random_state=4733, verbose=0, warm_start=False)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nprint(f\"F1 Score {f1_score(y_test, y_pred, average='macro')}\")\nprint(f\"Accuracy {accuracy_score(y_test, y_pred)}\")\nauc = roc_auc_score(y_test, y_pred)\nprint(\"AUC Score:\", auc)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-26T00:11:58.663609Z\",\"iopub.execute_input\":\"2023-05-26T00:11:58.664703Z\",\"iopub.status.idle\":\"2023-05-26T00:12:12.029867Z\",\"shell.execute_reply.started\":\"2023-05-26T00:11:58.664623Z\",\"shell.execute_reply\":\"2023-05-26T00:12:12.028788Z\"}}\nfrom lime.lime_tabular import LimeTabularExplainer\n\n# Initialize LimeTabularExplainer\nexplainer = LimeTabularExplainer(training_data=X_train.values,\n                                 feature_names=X_train.columns,\n                                 class_names=['0', '1'],\n                                 mode='classification')\n\n# Define the function to predict using the Random Forest model\npredict_fn = lambda x: rf.predict_proba(x)\n\n# Interpret the first 20 instances\nexplanation_first = explainer.explain_instance(X_test[:1].values[0],\n                                               predict_fn,\n                                               num_features=len(X_train.columns))\n\n# Interpret the last 20 instances\nexplanation_last = explainer.explain_instance(X_test[-1:].values[0],\n                                              predict_fn,\n                                              num_features=len(X_train.columns))\n\n# Show the explanation for the first 20 instances\nexplanation_first.show_in_notebook()\n\n# Show the explanation for the last 20 instances\nexplanation_last.show_in_notebook()\n\n\n# %% [markdown]\n# # Interpreting Using Shapley Values\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-25T23:40:45.775741Z\",\"iopub.execute_input\":\"2023-05-25T23:40:45.776186Z\",\"iopub.status.idle\":\"2023-05-25T23:40:46.344203Z\",\"shell.execute_reply.started\":\"2023-05-25T23:40:45.776151Z\",\"shell.execute_reply\":\"2023-05-25T23:40:46.341117Z\"}}\n# %% Create SHAP explainer\nexplainer = shap.TreeExplainer(rf)\n# Calculate shapley values for test data\nstart_index = 1\nend_index = 2\nshap_values = explainer.shap_values(X_test[start_index:end_index])\nX_test[start_index:end_index]\n\n# %% Investigating the values (classification problem)\n# class 0 = contribution to class 1\n# class 1 = contribution to class 2\nprint(shap_values[0].shape)\nshap_values\n\n# %% >> Visualize local predictions\nshap.initjs()\n# Force plot\nprediction = rf.predict(X_test[start_index:end_index])[0]\nprint(f\"The RF predicted: {prediction}\")\nshap.force_plot(explainer.expected_value[1],\n                shap_values[1],\n                X_test[start_index:end_index]) # for values\n\n# %% >> Visualize global features\n# Feature summary\nshap.summary_plot(shap_values, X_test)\n\n# %% [markdown]\n# # CounterFactual\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-26T00:58:51.791078Z\",\"iopub.execute_input\":\"2023-05-26T00:58:51.791487Z\",\"iopub.status.idle\":\"2023-05-26T00:59:03.462713Z\",\"shell.execute_reply.started\":\"2023-05-26T00:58:51.791457Z\",\"shell.execute_reply\":\"2023-05-26T00:59:03.461172Z\"}}\npip install dice-ml\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-05-26T01:27:22.735995Z\",\"iopub.execute_input\":\"2023-05-26T01:27:22.736940Z\"}}\n# %% Create diverse counterfactual explanations\n\nimport dice_ml\n# Dataset\ndata_dice = dice_ml.Data(dataframe=data_loader.data, \n                         # For perturbation strategy\n                         continuous_features=['TotalCharges', \n                                              'tenure',\n                                              'SeniorCitizen',\n                                              'MonthlyCharges'], \n                         outcome_name='Churn')\n# Model\nrf_dice = dice_ml.Model(model=rf, \n                        # There exist backends for tf, torch, ...\n                        backend=\"sklearn\")\nexplainer = dice_ml.Dice(data_dice, \n                         rf_dice, \n                         # Random sampling, genetic algorithm, kd-tree,...\n                         method=\"random\")\n\n# %% Create explanation\n# Generate CF based on the blackbox model\ninput_datapoint = X_test[0:1]\ncf = explainer.generate_counterfactuals(input_datapoint, \n                                  total_CFs=3, \n                                  desired_class=\"opposite\")\n# Visualize it\ncf.visualize_as_dataframe(show_only_changes=True)\n\n\n\n\n# %% [code]\n# %% Create feasible (conditional) Counterfactuals\nfeatures_to_vary=['TotalCharges',\n                  'tenure',\n                  'Contract_Month-to-month']\npermitted_range={'avg_glucose_level':[50,250],\n                'bmi':[18, 35]}\n# Now generating explanations using the new feature weights\ncf = explainer.generate_counterfactuals(input_datapoint, \n                                  total_CFs=3, \n                                  desired_class=\"opposite\",\n                                  permitted_range=permitted_range,\n                                  features_to_vary=features_to_vary)\n# Visualize it\ncf.visualize_as_dataframe(show_only_changes=True)","metadata":{"_uuid":"91fc7561-69f6-4267-baca-69d396a13de9","_cell_guid":"8639089e-c225-49da-8323-3f787201452f","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}